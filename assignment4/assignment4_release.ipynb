{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw6n8sqlFIjZ",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Assignment 4: Self-Attention for Vision\n",
    "\n",
    "For this assignment, we're going to implement self-attention blocks in a convolutional neural network for CIFAR-10 Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWmQwMDlFIjj"
   },
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhcP-S1VFIjj",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134,
     "referenced_widgets": [
      "6646a364fe5b431eb578707bae8329b8",
      "92a914b698524f5ebd49603d108fbff9",
      "149c512369044de1951159ab8d9ec5ed",
      "15e4679b552e4d9c85a490b59222d1cf",
      "e7cb787fa0d743c08b5d4c395a7431f7",
      "e3796ddf7334425dbc86e68ff37d8a18",
      "d080590971d14b158aea92032dfebecc",
      "4dfce8d8475644e69a52e20410e4fd1b"
     ]
    },
    "id": "ZJJBVo0YFIjk",
    "outputId": "0ad37cdb-9bab-4f74-bdc5-0463ab7cbbad",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./data/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An3cfS0tFIjm",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOu2LLoEFIjm",
    "outputId": "683549ab-f649-4709-fd83-d44ef527e4df",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0Cp2ir0FIjn",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### PyTorch Tensors: Flatten Function\n",
    "A PyTorch Tensor is conceptionally similar to a numpy array: it is an n-dimensional grid of numbers, and like numpy PyTorch provides many functions to efficiently operate on Tensors. As a simple example, we provide a `flatten` function below which reshapes image data for use in a fully-connected neural network.\n",
    "\n",
    "Recall that image data is typically stored in a Tensor of shape N x C x H x W, where:\n",
    "\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we use fully connected affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `C x H x W` values per representation into a single long vector. The flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4QkOBJdFIjo",
    "outputId": "286fe612-3115-42ad-a8aa-352adb9d74e2",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGHNkBu-FIjr"
   },
   "source": [
    "### Check Accuracy Function\n",
    "When training the model we will use the following function to check the accuracy of our model on the training or validation sets.\n",
    "\n",
    "When checking accuracy we don't need to compute any gradients; as a result we don't need PyTorch to build a computational graph for us when we compute scores. To prevent a graph from being built we scope our computation under a `torch.no_grad()` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeM908EmFIjv"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # useful stateless functions\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return 100 * acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ma0ntxxFIjv"
   },
   "source": [
    "### Training Loop\n",
    "We also use a slightly different training loop. Rather than updating the values of the weights ourselves, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kP6SeaiFIjv"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    acc_max = 0\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                acc = check_accuracy(loader_val, model)\n",
    "                if acc >= acc_max:\n",
    "                    acc_max = acc\n",
    "                print()\n",
    "    print(\"Maximum accuracy attained: \", acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla CNN; No Attention\n",
    "We implement the vanilla architecture for you here. Do not modify the architecture. You will use the same architecture in the following parts. Do not modify the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, 3, padding=1, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
    "You should be able to see atleast 55% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "vanillaModel = model\n",
    "check_accuracy(loader_test, vanillaModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gH8-iqGUOO1"
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "In the next section, you will implement an Attention layer which you will then use within a convnet architecture defined above for cifar 10 classification task.\n",
    "\n",
    "A self-attention layer is formulated as following:\n",
    "\n",
    "Input: $X$ of shape $(H\\times W, C)$\n",
    "\n",
    "Query, key, value linear transforms are $W_Q$, $W_K$, $W_V$, of shape $(C, C)$. We implement these linear transforms as 1x1 convolutional layers of the same dimensions.\n",
    "\n",
    "$XW_Q$, $XW_K$, $XW_V$, represent the output volumes when input X is passed through the transforms.\n",
    "\n",
    "\n",
    "Self-Attention is given by the formula: $Attention(X) = X + Softmax(\\frac{XW_Q(XW_K)^\\top}{\\sqrt{C}})XW_V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 1: Self-Attention is equivalent to which of the following: (10% of the grade)\n",
    "1. K-means clustering <br />\n",
    "2. Non-local means <br />\n",
    "3. Residual Block <br />\n",
    "4. Gaussian Blurring <br />\n",
    "\n",
    "Your Answer: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you implement the Attention module, and run it in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvcTf4eUY-FX"
   },
   "outputs": [],
   "source": [
    "# Initialize the attention module as a nn.Module subclass\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Implement the Key, Query and Value linear transforms as 1x1 convolutional layers\n",
    "        # Hint: channel size remains constant throughout\n",
    "        self.conv_query = \n",
    "        self.conv_key = \n",
    "        self.conv_value = \n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        # TODO: Pass the input through conv_query, reshape the output volume to (N, C, H*W)\n",
    "        q = \n",
    "        # TODO: Pass the input through conv_key, reshape the output volume to (N, C, H*W)\n",
    "        k = \n",
    "        # TODO: Pass the input through conv_value, reshape the output volume to (N, C, H*W)\n",
    "        v = \n",
    "        # TODO: Implement the above formula for attention using q, k, v, C\n",
    "        # NOTE: The X in the formula is already added for you in the return line\n",
    "        attention = \n",
    "        # Reshape the output to (N, C, H, W) before adding to the input volume\n",
    "        attention = attention.reshape(N, C, H, W)\n",
    "        return x + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Attention Block: Early attention; After the first conv layer. (50% of the grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6A3xh7aVFWzM",
    "outputId": "f45f9049-70ce-414f-ab1d-446f619ad367"
   },
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the first Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Linear]\n",
    "\n",
    "model = None\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
    "You should see improvement of about 2-3% over the vanilla convnet model. * Use this part to tune your Attention module and then move on to the next parts. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "earlyAttention = model\n",
    "check_accuracy(loader_test, earlyAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Attention Block: Late attention; After the second conv layer. (10% of the grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Conv->Relu->Attention->Relu->Linear]\n",
    "\n",
    "model = None\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "lateAttention = model\n",
    "check_accuracy(loader_test, lateAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 2: Provide one example each of usage of self-attention and attention in computer vision. Explain the difference between the two. ( 10% of the grade)\n",
    "\n",
    "\n",
    "Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Attention Blocks: After conv layers 1 and 2 (10% of the grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Attention->Relu->Linear]\n",
    "\n",
    "model = None\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "vanillaModel = model\n",
    "check_accuracy(loader_test, vanillaModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 3: Rank the above models based on their performance on test dataset (20% of the grade)\n",
    "( You are encouraged to run each of the experiments (training) atleast 3 times to get an average estimate )\n",
    "\n",
    "Report the test accuracies alongside the model names. For example, 1. Vanilla CNN (57.45%, 57.99%).. etc\n",
    "\n",
    "1. <br />\n",
    "2. <br />\n",
    "3. <br />\n",
    "4. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question (Ungraded): Can you give a possible explanation that supports the rankings?\n",
    "Your Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "assignment3_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "149c512369044de1951159ab8d9ec5ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3796ddf7334425dbc86e68ff37d8a18",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7cb787fa0d743c08b5d4c395a7431f7",
      "value": 170498071
     }
    },
    "15e4679b552e4d9c85a490b59222d1cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dfce8d8475644e69a52e20410e4fd1b",
      "placeholder": "​",
      "style": "IPY_MODEL_d080590971d14b158aea92032dfebecc",
      "value": " 170499072/? [00:18&lt;00:00, 9456903.71it/s]"
     }
    },
    "4dfce8d8475644e69a52e20410e4fd1b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6646a364fe5b431eb578707bae8329b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_149c512369044de1951159ab8d9ec5ed",
       "IPY_MODEL_15e4679b552e4d9c85a490b59222d1cf"
      ],
      "layout": "IPY_MODEL_92a914b698524f5ebd49603d108fbff9"
     }
    },
    "92a914b698524f5ebd49603d108fbff9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d080590971d14b158aea92032dfebecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3796ddf7334425dbc86e68ff37d8a18": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7cb787fa0d743c08b5d4c395a7431f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}