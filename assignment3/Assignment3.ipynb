{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Pytorch Segmentation\n",
    "For this assignment, we're goining to use Deep Learning for a new task: semantic segmentation , instead of classification we've been doing. We will also use some common techniques in Deep Learning like pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short recap of  semantic segmentation\n",
    "The goal of semantic segmentation is to classify each pixel of the image to a corresponding class of what the pixel represent. One major deference between semantic segmentation and classification is that for semantic segmentation, model output label for each pixel instead of a single label for the whole image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "In semantic segmentations, we will average pixel-wise accuracy and IoU to benchmark semantic segmentation methods. Here we provide, the code for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _hist(pred, gt, n_class):\n",
    "#     mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * gt.astype(int) +\n",
    "        pred, minlength=n_class ** 2\n",
    "    ).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def metrics(preds, gts, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for pred, gt in zip(preds, gts):\n",
    "        hist += _hist(pred.flatten(), gt.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    iou = np.diag(hist) / (\n",
    "        hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
    "    )\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    return acc, mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMP Facade DB\n",
    "In this assignment, we use a new dataset named: CMP Facade Database for semantic segmentation. This dataset is made up with 606 rectified images of the facade of various buildings. The facades are from different cities arount the world with different architectural styles.\n",
    "\n",
    "CMP Facade DB include 12 semantic classes:\n",
    "\n",
    "* facade \n",
    "* molding\n",
    "* cornice\n",
    "* pillar\n",
    "* window\n",
    "* door\n",
    "* sill\n",
    "* blind\n",
    "* balcony\n",
    "* shop\n",
    "* deco\n",
    "* background\n",
    "\n",
    "In this assignment, we should use a model to classify each pixel in images to one of these 12 classes.\n",
    "\n",
    "For more detail about CMP Facade Dataset, if you are intereseted, please check: https://cmp.felk.cvut.cz/~tylecr1/facade/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PI\n",
    "\n",
    "idxs = [1, 2, 5, 6, 7, 8]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    pic = plt.imread(\"dataset/base/cmp_b000{}.jpg\".format(idx))\n",
    "    label = plt.imread(\"dataset/base/cmp_b000{}.png\".format(idx), format=\"PNG\")\n",
    "\n",
    "    axes[0][i].axis('off')\n",
    "    axes[0][i].imshow(pic)\n",
    "    axes[0][i].set_title(\"Raw Image\")\n",
    "\n",
    "    axes[1][i].imshow(label)\n",
    "    axes[1][i].axis('off')\n",
    "    axes[1][i].set_title(\"Ground Truth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset Class in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_full_list(\n",
    "    root_dir,\n",
    "    base_dir=\"base\",\n",
    "    extended_dir=\"extended\",\n",
    "):\n",
    "    data_list = []\n",
    "    for name in [base_dir, extended_dir]:\n",
    "        data_dir = osp.join(\n",
    "            root_dir, name\n",
    "        )\n",
    "        data_list += sorted(\n",
    "            osp.join(data_dir, img_name) for img_name in\n",
    "            filter(\n",
    "                lambda x: x[-4:] == '.jpg',\n",
    "                os.listdir(data_dir)\n",
    "            )\n",
    "        )\n",
    "    return data_list\n",
    "\n",
    "class CMP_Facade_DB(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_list\n",
    "    ):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "                \n",
    "    def __getitem__(self, i):\n",
    "        # input and target images\n",
    "        in_name = self.data_list[i]\n",
    "        gt_name = self.data_list[i].replace('.jpg','.png')\n",
    "    \n",
    "        # process the images\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        transf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        in_image = transf(\n",
    "            Image.open(in_name).convert('RGB')\n",
    "        )\n",
    "        gt_im = Image.open(gt_name)\n",
    "        \n",
    "        gt_label = torch.LongTensor(\n",
    "            np.frombuffer(gt_im.tobytes(), dtype=np.ubyte).reshape(\n",
    "                in_image.shape[1:]\n",
    "            )\n",
    "        ) - 1\n",
    "\n",
    "        return in_image, gt_label\n",
    "    \n",
    "    def revert_input(self, img, label):\n",
    "        img = np.transpose(img.cpu().numpy(), (1, 2, 0))\n",
    "        std_img = np.array([0.229, 0.224, 0.225]).reshape((1, 1, -1))\n",
    "        mean_img = np.array([0.485, 0.456, 0.406]).reshape((1, 1, -1))\n",
    "        img *= std_img\n",
    "        img += mean_img\n",
    "        label = label.cpu().numpy()\n",
    "        return img, label + 1\n",
    "\n",
    "TRAIN_SIZE = 500\n",
    "VAL_SIZE = 30\n",
    "TEST_SIZE = 70\n",
    "full_data_list = get_full_list(\"dataset\")\n",
    "\n",
    "train_data_set = CMP_Facade_DB(full_data_list[: TRAIN_SIZE])\n",
    "val_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE: TRAIN_SIZE + VAL_SIZE])\n",
    "test_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE + VAL_SIZE:])\n",
    "\n",
    "print(\"Training Set Size:\", len(train_data_set))\n",
    "print(\"Validation Set Size:\", len(val_data_set))\n",
    "print(\"Test Set Size:\", len(test_data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks for Semantic Segmentation\n",
    "\n",
    "We've seen that CNNs are powerful models to get hiereachical visual features in Deep Learning. There we are going to explore the classical work: \"Fully Convolutional Networks for Semantic Segmentation\"(FCN).\n",
    "\n",
    "Though we've already used CNN models for image classifications in the previous assignemtn, those models have one major drawback: Those model take input with fixed shape and output a single vector. However, in semantic segmentation, we want the model to be able to process image with arbitary shape and predict the label map with the same shape as the input image.\n",
    "\n",
    "In FCN, the model utilize the Transpose Convolution layers, which we've already learned during the lecture, to make it happen. For the overal introduction of Transpose Convolution and Fully Convolutional Networks, please review the lecture recording and lecture slides on Canvas(Lecture 10).\n",
    "\n",
    "Here we do not cover all the details in FCN. If you need more reference, you can check the original paper: https://arxiv.org/pdf/1411.4038.pdf and some other materials online.\n",
    "\n",
    "Besides of transpose Convolution, there are also some difference compared with the models we've been working on:\n",
    "\n",
    "* Use 1x1 Convolution to replace fully connected layers to output score for each class.\n",
    "* Use skip connection to combine high-level feature and local feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive FCN: FCN-32s (30%)\n",
    "\n",
    "In this section, we first try to implement naive variant of FCN without skip connection: FCN-32s. Here we use FCN-32s with VGG-16 architecture for feature encoding.\n",
    "\n",
    "Compared with VGG-16, FCN-32s only replace the fully connecteed layers with 1x1 convolution and add a Transpose Convolution at the end to output dense prediction.\n",
    "\n",
    "\n",
    "FC-32s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1(except conv1_1. conv1_1 should use padding = 100)\n",
    "The Max Pool should use \"ceil_mode = True\"\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] ->  [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] ->  [maxpool3(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] ->  [maxpool3(2,2)]\n",
    "\n",
    "\n",
    "The following Conv use kernel size = 7, stride = 1, padding = 0\n",
    "* [fc6=conv(512, 4096, 7)-relu-dropout2d]\n",
    "\n",
    "The following Conv use kernel size = 1, stride = 1, padding = 0\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The transpose convolution: kernal size = 64, stride = 32, bias = False\n",
    "* [transpose_conv(n_class, n_class)]\n",
    "\n",
    "**Note: The output of the transpose convlution might not have the same shape as the input, take [19: 19 + input_image_width], [19: 19 + input_image_height] for width and height dimension of the output to get the output with the same shape as the input**\n",
    "\n",
    "**It's expected that you model perform very poor in this section. This section is mainly for you to debug your code**\n",
    "\n",
    "**Try to name the layers use the name provide above to ensure the next section works correctly, and use a new nn.RELU() for each activation**\n",
    "\n",
    "**The model should achieve at least 40% Mean IoU for the pretrained version.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, n_class=12):\n",
    "        super(FCN32s, self).__init__()\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO: Implement the layers for FCN32s.                                       #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 m.weight.data.zero_()\n",
    "#                 if m.bias is not None:\n",
    "#                     m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN32s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        \n",
    "        return h\n",
    "\n",
    "    \n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
    "            l2.bias.data = l1.bias.data.view(l2.bias.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change it if you want\n",
    "lr = 1e-4\n",
    "weight_decay = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_set, batch_size=1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Evaluate(\n",
    "    val_loader,\n",
    "    model,\n",
    "    current_best,\n",
    "    n_class=12\n",
    "):\n",
    "    val_loss = 0\n",
    "    visualizations = []\n",
    "    preds, gts = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            score = model(data)\n",
    "\n",
    "        pred = score.max(1)[1].cpu().numpy()\n",
    "        gt = target.cpu().numpy()\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "\n",
    "    avg_acc, mean_iou = metrics(\n",
    "        preds, gts, n_class)\n",
    "\n",
    "    if mean_iou > current_best[\"IoU\"]:\n",
    "        current_best[\"IoU\"] = mean_iou\n",
    "        current_best[\"model\"] = copy.deepcopy(model)\n",
    "\n",
    "    return avg_acc, mean_iou, current_best\n",
    "\n",
    "def Train(\n",
    "    model,\n",
    "    loss_func,\n",
    "    optim,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    train_loader,\n",
    "    val_lodaer,\n",
    "    test_loader,\n",
    "    display_interval = 100\n",
    "):\n",
    "\n",
    "    current_best = {\n",
    "        \"IoU\": 0,\n",
    "        \"model\": model\n",
    "    }\n",
    "    avg_acc, mean_iou, current_best = Evaluate(\n",
    "        val_loader,\n",
    "        model,\n",
    "        current_best\n",
    "    )\n",
    "    \n",
    "    print(\"Init Model\")\n",
    "    print(\"Avg Acc: {:.4}, Mean IoU: {:.4}\".format(\n",
    "        avg_acc, mean_iou\n",
    "    ))\n",
    "    for i in range(epochs):\n",
    "        print(\"Epochs: {}\".format(i))\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(\"cuda:0\"), target.to(\"cuda:0\")\n",
    "            optim.zero_grad()\n",
    "\n",
    "            score = model(data)\n",
    "            loss = loss_func(score, target.squeeze(1))\n",
    "            loss_data = loss.item()\n",
    "            if np.isnan(loss_data):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % display_interval == 0 and batch_idx != 0:\n",
    "                print(\"{} / {}, Current Avg Loss:{:.4}\".format(\n",
    "                    batch_idx, len(train_loader), total_loss / (batch_idx + 1)\n",
    "                ))\n",
    "            \n",
    "        \n",
    "        total_loss /= len(train_loader)\n",
    "        model.eval()\n",
    "        avg_acc, mean_iou, current_best = Evaluate(\n",
    "            val_loader,\n",
    "            model,\n",
    "            current_best\n",
    "        )\n",
    "        scheduler.step(total_loss)\n",
    "        print(\"Avg Loss: {:.4}, Avg Acc: {:.4}, Mean IoU: {:.4}\".format(\n",
    "            total_loss, avg_acc, mean_iou\n",
    "        ))\n",
    "    \n",
    "    test_acc, test_iou, current_best = Evaluate(\n",
    "        val_loader,\n",
    "        current_best[\"model\"],\n",
    "        current_best\n",
    "    )\n",
    "    print(\"Test Acc: {:.4}, Test Mean IoU: {:.4}\".format(\n",
    "        test_acc, test_iou\n",
    "    ))\n",
    "    return current_best[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN32s(n_class=12)\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optim, 'min', patience=3,\n",
    "    min_lr=1e-10, verbose=True\n",
    ")\n",
    "\n",
    "# Choose the right loss function in torch.nn\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "best_model = Train(\n",
    "    model,\n",
    "    loss_func,\n",
    "    optim,\n",
    "    scheduler,\n",
    "    5,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Output\n",
    "In this section, we visualize several model outputs to see how our model actually perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, test_loader):\n",
    "    idxs = [1, 2, 5, 6, 7, 8]\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(12, 8))\n",
    "    model.eval()\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, label = test_loader.dataset[idx]\n",
    "        \n",
    "        pred = model(img.unsqueeze(0).to(device)) \n",
    "        pred = (pred.max(1)[1] + 1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        img, label = test_loader.dataset.revert_input(img, label)\n",
    "        \n",
    "        axes[0][i].axis('off')\n",
    "        axes[0][i].imshow(img)\n",
    "        axes[0][i].set_title(\"Raw Image\")\n",
    "\n",
    "        axes[1][i].imshow(label)\n",
    "        axes[1][i].axis('off')\n",
    "        axes[1][i].set_title(\"Ground Truth\")\n",
    "\n",
    "        axes[2][i].imshow(pred)\n",
    "        axes[2][i].axis('off')\n",
    "        axes[2][i].set_title(\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize the pretrain features\n",
    "\n",
    "In the previous section, we use the random initalized weights to train FCN-32S from scrath. We can see that it perform poorly. In this section, we utilize the feature from pretrained model(In our case, we use VGG-16) to help us get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model = FCN32s(n_class=12)\n",
    "model.copy_params_from_vgg16(vgg16)\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optim, 'min', patience=3,\n",
    "    min_lr=1e-10, verbose=True\n",
    ")\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "best_model_pretrain = Train(\n",
    "    model,\n",
    "    loss_func,\n",
    "    optim,\n",
    "    scheduler,\n",
    "    25,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model_pretrain, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Connection: FCN-8s(40%)\n",
    "\n",
    "Though we've get a prety good result using FCN-32s with VGG-16 pretrain. We can actully do better with another technique introduced in FCN paper: Skip Connection. \n",
    "\n",
    "With skip connection, we are supposed to get a better performance especially for some details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the structure of FCN-8s, the variant of FCN with skip connections.\n",
    "\n",
    "FCN-8s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1(except conv1_1. conv1_1 should use padding = 100)\n",
    "The Max Pool should use \"ceil_mode = True\"\n",
    "\n",
    " **As you can see, the structure of this part is the same as FCN-32s**\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] -> [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] -> [maxpool3(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] -> [maxpool3(2,2)]\n",
    "\n",
    "The following Conv use kernel size = 7, stride = 1, padding = 0\n",
    "* [fc6=conv(512, 4096, 7)-relu-dropout2d]\n",
    "\n",
    "The following Conv use kernel size = 1, stride = 1, padding = 0\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The Additional Score Pool use kernel size = 1, stride = 1, padding = 0\n",
    "* [score_pool_3 =conv1x1(256, num_classes)]\n",
    "* [score_pool_4 =conv1x1(512, num_classes)]\n",
    "\n",
    "The transpose convolution: kernel size = 4, stride = 2, bias = False\n",
    "* [upscore1 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernel size = 4, stride = 2, bias = False\n",
    "* [upscore2 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernel size = 16, stride = 8, bias = False\n",
    "* [upscore3 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "Different from FCN-32s which has only single path from input to output, there are multiple data path from input to output in FCN-8s.\n",
    "\n",
    "The following graph is from original FCN paper, you can also find the graph there.\n",
    "\n",
    "![\"Architecture Graph\"](arch.png)\n",
    "\"Layers are shown as grids that reveal relative spatial coarseness. Only pooling and prediction layers are shown; intermediate convolution layers (including converted fully connected layers) are omitted. \" ---- FCN\n",
    "\n",
    "Detailed path specification:\n",
    "\n",
    "* score_pool_3\n",
    "    * input: output from layer \"pool3\"\n",
    "    * take [9: 9 + upscore2_width], [9: 9 + upscore2_height]\n",
    "    \n",
    "* score_pool_4,\n",
    "    * input: output from layer \"pool4\"\n",
    "    * take [5: 5 + upscore1_width], [5: 5 + upscore1_height]\n",
    "\n",
    "\n",
    "* upscore1\n",
    "    * input: output from layer \"score\"\n",
    "\n",
    "* upscore2:\n",
    "    * input: output from layer \"score_pool_4\" + output from layer \"upscore1\"\n",
    "\n",
    "* upscore3:\n",
    "    * input: output from layer \"score_pool_3\" + output from layer \"upscore2\"\n",
    "    * take [31: 31 + input_image_width], [31: 31 + input_image_height]\n",
    "\n",
    "\n",
    "**The model should achieve at least 40% Mean IoU**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=12):\n",
    "        super(FCN8s, self).__init__()\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the layers for FCN8s.                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            # if isinstance(m, nn.Conv2d):\n",
    "            #     m.weight.data.zero_()\n",
    "            #     if m.bias is not None:\n",
    "            #         m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN8s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "\n",
    "        return h\n",
    "\n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data.copy_(l1.weight.data)\n",
    "                l2.bias.data.copy_(l1.bias.data)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model = FCN8s(n_class=12)\n",
    "model.copy_params_from_vgg16(vgg16)\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "#     momentum=momentum,\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optim, 'min', patience=3,\n",
    "    min_lr=1e-10, verbose=True\n",
    ")\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "best_model_fcn8s = Train(\n",
    "    model,\n",
    "    loss_func,\n",
    "    optim,\n",
    "    scheduler,\n",
    "    25,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(best_model_fcn8s, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Questions(30%):\n",
    "\n",
    "#### Inline Question 1: Why using pretrained model to initialized our model(FCN-32s) helps a lot? Please give at least two specific reasons\n",
    "\n",
    "### Your Answer:\n",
    "\n",
    "\n",
    "#### Inline Question 2: Compare the performance and visualization of FCN-32s and FCN-8s. Please state the differnece, and provide some explanation. You can visualize more images than we provide, if it's necessary for you to see the difference.\n",
    "\n",
    "### Your Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}